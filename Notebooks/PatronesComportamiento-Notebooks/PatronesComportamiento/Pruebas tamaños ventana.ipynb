{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24381,
     "status": "ok",
     "timestamp": 1680186180740,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "VlE01Q5x2PRs",
    "outputId": "df8d2111-9f32-4f59-fac7-dea30f89385d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: skops in /home/ana/.local/lib/python3.8/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ana/.local/lib/python3.8/site-packages (from skops) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/ana/.local/lib/python3.8/site-packages (from skops) (1.1.3)\n",
      "Requirement already satisfied: tabulate>=0.8.8 in /home/ana/.local/lib/python3.8/site-packages (from skops) (0.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.1 in /home/ana/.local/lib/python3.8/site-packages (from skops) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.10.1->skops) (5.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.10.1->skops) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ana/.local/lib/python3.8/site-packages (from huggingface-hub>=0.10.1->skops) (4.4.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ana/.local/lib/python3.8/site-packages (from huggingface-hub>=0.10.1->skops) (4.65.0)\n",
      "Requirement already satisfied: requests in /home/ana/.local/lib/python3.8/site-packages (from huggingface-hub>=0.10.1->skops) (2.18.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ana/.local/lib/python3.8/site-packages (from packaging>=17.0->skops) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ana/.local/lib/python3.8/site-packages (from scikit-learn>=0.24->skops) (1.23.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ana/.local/lib/python3.8/site-packages (from scikit-learn>=0.24->skops) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ana/.local/lib/python3.8/site-packages (from scikit-learn>=0.24->skops) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/ana/.local/lib/python3.8/site-packages (from scikit-learn>=0.24->skops) (1.2.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/ana/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.10.1->skops) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/ana/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.10.1->skops) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ana/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.10.1->skops) (2022.6.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.10.1->skops) (3.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install skops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680187102098,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "lwh4ZnDY8fdv"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_percentage_error,mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import skops.io as io\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmqoXToSuFKT"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2TobWEYuKAS"
   },
   "source": [
    "Estas configuraciones de ventana se harán en primer lugar para el modelo RandomForest.\n",
    "Random Forest es una de los algoritmos más populares y más comunmente usados por científicos del Dato. Random forest es un algorítmo de Machine Learning supervisado que es ampliamente utilizado y problemas de regresión y clasificación. Este crea árboles de decisión a partir de diferentes muestras y toma su voto mayoritario para clasificación y su aproximación para regresión.\n",
    "\n",
    "Una de las características más importantes del algoritmo Random Forest es que puede soportar data sets con variables contínuas, como es el caso de la regresión, y variables categóricas como es el caso de la clasificación.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9JuxR5X2u6c"
   },
   "source": [
    "El sobreajuste suele ser un problema crítico que puede empeorar los resultafos obtenidos pero para el algortimo random forest, si hay suficientes árboles en el bosque el casificador con reajustará el modelo. Otro punto a favor de este algoritmo es que puede soportar valores vacíos, no obstante para nuestro ejemplo no trataremos con ellos pues como se muestra en el jupyter [Transformación de Datos](https://colab.research.google.com/drive/13Bu4aIENRpZTCj9K0ozJAoSONLPTK5jI?usp=sharing), se han reconstruido todos aquellos tramos horarios en los que había cortes de luz y se perdían muetras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcsNcwKh4Aiu"
   },
   "source": [
    "# Cómo funciona este algoritmo\n",
    "Para explicarlo nos basaremos en la siguiente ilustación. Como se observa en la imagen inferior observamos como a partir de los dígitos iniciales que vienen subrayados y en distintos colores el árbol va tomando decisiones y categorizándolo. Tenemos el siguiente número 1 1 0 0 0 0 0, que presenta distintos colores y algunos de ellos están subrayados y otros no. Queremos categorizar dicho número en: rojo, azul y subrayado. El funcionamiento de un árbol de decisión viene definido por un esquema, la primera pregunta clasificará a aquellos números que sean de color rojo por una lado y los de otro color por otro, para la siguiente clasificación solo se tiene en cuenta el conjunto de la izquierda pues el de la derecha ya ha llegado a una categorá final: azul. Siguiendo con el conjunto de la izquierda podemos categorizar en subrayado y no subrayado, por tanto tendremos a la izquierda los unos subrayados y a la derecha el cero sin subrayar. Para nuestro problema es algo más complejo pero entendiendo las bases se puede estrapolar perfectamente a casos de mayor embergadura.\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1hoYiU6y5bpVt6yS51cdKbHUfxiDzMBP4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2frIZPyZgDa8"
   },
   "source": [
    "Pero en este caso no estaríamos hablando de un único árbol de decisión si no de muchos árboles que conformarán 'El bosque', de ahí el nombre del algoritmo. Estos arbolitos trabajan como un conjunto y este comportamiento es la clave de su  baja correlación y en consecuencia su mejor rendimiento frente a algorítmos con una alta correlación. Cada árbol da una predicción de forma que el impacto de la predicción se divide entre el número de árboles totales, de forma que la predicción final está protegida de los pequeños errores individuales. Uso por tanto la sabiduría de masas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BEkgxbihiWN"
   },
   "source": [
    "# Transformar serie temporal en ventanas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIdKpevWR1pu"
   },
   "source": [
    "Iremos modificando los tamaños de ventana y comprobando que resultados se van obteniendo tras entrenar el modelo con la mejor configuración de validación cruzada. Primero tomaremos con pickle la serie que contiene todas las series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1710,
     "status": "ok",
     "timestamp": 1680186241906,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "U4E5JuAHP8_4",
    "outputId": "e945871a-f67f-4215-b59d-075e1de4e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.288288, 1.207547, 1.316038, 1.264151, 1.132075, 0.8962264, 1.042453, 0.6650943, 0.8207547, 0.9009434]\n",
      "287\n"
     ]
    }
   ],
   "source": [
    "with open('mySeriesInten.pkl', 'rb') as file:\n",
    "    timeSeries = pickle.load(file)\n",
    "\n",
    "with open('singleSerieIntenTrT.pkl', 'rb') as file:\n",
    "    singleSerieTrT = pickle.load(file)\n",
    "\n",
    "\n",
    "with open('singleSerieIntenValit.pkl', 'rb') as file:\n",
    "    singleSerieValit= pickle.load(file)\n",
    "\n",
    "print((singleSerieTrT[:10]))\n",
    "print(len(timeSeries[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kO5ShdQgsE7"
   },
   "source": [
    "Lo primero que vamos a hacer es escalar los datos con sk learn en su paquete Preproces[Preprocesing](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) tiene alguns métodos que permiten escalar los datos de entrenamiento según distintos criterios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1680186241907,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "pJZH_UnpgrUl"
   },
   "outputs": [],
   "source": [
    "def scale(data):  \n",
    "  min_max_scaler = preprocessing.MinMaxScaler()\n",
    "  X_train=np.array(data)\n",
    "  X_train = X_train.reshape((len(data), 1))\n",
    "  X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "  X_train_minmax=[ele[0] for ele in X_train_minmax]\n",
    "  return X_train_minmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp0WE7Tch6b4"
   },
   "source": [
    "En primer lugar aplicaremos una ventana a los datos, en este caso la ventana será de 5 y la h de 1. La forma que hemos elegido para aplicar la ventana deslizante es a través de sliding vectorial, posteriomente se añade a un dataframe de pandas. A continuación en la celda contigua podemos ver la definición del método que aplicará la ventana deslizante. Para entender el por qué de utilizar esta técnica ir al jupyter [Transformar datos para prediccion](https://colab.research.google.com/drive/1HuCj1Cetas3ffaXV8q83ynGQaV_njtI6?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680186243820,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "G7TLr3aiTb1T"
   },
   "outputs": [],
   "source": [
    "def spliting_timeseries(timeSerie,w,h):  #w amplitud de la ventana y h >1 si es multiobjetivo\n",
    "  X, y = [], []  #inicializamos los arrays que guardarán atributos y variable predicha\n",
    "  for i in range(len(timeSerie) - w): #bucle para iterar sobre todas las muestras de las series \n",
    "    X.append(timeSerie[i : i + w])  #cogemos desde el elemento en el que estamos hasta los w siguientes\n",
    "    y.append(timeSerie[i + w:i+w+h])  #aquí cogemos desde el w+1 hasta los h siguientes\n",
    "  return pd.DataFrame(X), pd.Series(y),X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt_JVpjLif_J"
   },
   "source": [
    "Lo siguiente será llamar al método con aquellos parámetros con los que queramos configurar la ventana deslizante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W8jxq-5BTgq7"
   },
   "outputs": [],
   "source": [
    "X_train_minmax=scale(singleSerieTrT)\n",
    "cond,consec=[],[]\n",
    "x,y,x_train,y_train=spliting_timeseries(X_train_minmax,5,1) #llamo al método\n",
    "cond.append(x)  #lo añado a una lista de atributos\n",
    "  # lo añado a una lista de variables predichas\n",
    "\n",
    "\n",
    "\n",
    "data_cond = pd.concat(cond, axis=0) # combino todos los DataFrames en uno solo\n",
    "data_consec = pd.concat([y], axis=0) # combino todos los DataFrames en uno solo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjGiwxXfjEOQ"
   },
   "source": [
    "El siguiente paso a abordar será dividir los datos en entrenamiento y test. Algo que debemos tener en cuenta a la hora de hacer esta división es que nuestro objeto de estudio son series temporales, por tanto debemos plantear una for de dividir los datos sin que afecte a la línea temporal de los mismos. (Explicar con detalle cuando lea información al respecto).\n",
    "Para que la división utilizada respete la cronología se utilizará el método: [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit) de la librería SK Learn. Este método permite aplicar validación cruzada a series temporales respectando la cronología, Para este método si k es el número de pligues devolverá el pliegue k como el último conjunto que formaŕá parte del conjunto de entrenamiento y el k+1 como el primero del conjunto de test. A diferencia de los métodos cotidianos de validación cruzada los conjuntos sucesivos de entrenamiento son superconjuntos de los conjuntos de entrenamiento previos. Através de este método obtendrems los índices de los conjuntos que formarán parte del entrenamiento y de los que formarán parte del test.\n",
    "De entre los dintintos parámtros configurables en le método tan sólo configuraremos n_splits a 5, es meramente informativo pues es el valor por defecto del parámetro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679951787954,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "sZSO0FGXTjZ2",
    "outputId": "4e316b0f-c5f4-4bbb-85e2-b0c59310518e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 29400 29401 29402]\n",
      "  Test:  index=[29403 29404 29405 ... 58802 58803 58804]\n",
      "Fold 1:\n",
      "  Train: index=[    0     1     2 ... 58802 58803 58804]\n",
      "  Test:  index=[58805 58806 58807 ... 88204 88205 88206]\n",
      "Fold 2:\n",
      "  Train: index=[    0     1     2 ... 88204 88205 88206]\n",
      "  Test:  index=[ 88207  88208  88209 ... 117606 117607 117608]\n",
      "Fold 3:\n",
      "  Train: index=[     0      1      2 ... 117606 117607 117608]\n",
      "  Test:  index=[117609 117610 117611 ... 147008 147009 147010]\n",
      "Fold 4:\n",
      "  Train: index=[     0      1      2 ... 147008 147009 147010]\n",
      "  Test:  index=[147011 147012 147013 ... 176410 176411 176412]\n",
      "0    0.025950\n",
      "1    0.025034\n",
      "2    0.026264\n",
      "3    0.025676\n",
      "4    0.024178\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "s=5\n",
    "tscv = TimeSeriesSplit(n_splits=s)\n",
    "print(tscv)\n",
    "\n",
    "fols_dict={}\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(data_cond)):\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    fols_dict[i]=[train_index,test_index]\n",
    "    \n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "print(data_cond.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJJHbUilsNTZ"
   },
   "source": [
    "Ahora tenemos distintos conjuntos de entrenamiento y test, de cara al siguiente paso tendremos que tener elegida una configuaración, para ello calcularemos el score que se obtiene para cada una de ellas y aquel que consiga un mayor valor será selecionado. No obstante, aún quedan configuraciones que hacer antes de conseguir el conjunto de entrenamiento y test así como el modelo final.\n",
    "Tenemos que configurar los distintos conjuntos de entrenamiento y test y probar su rendimiento con la fución score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1679951787955,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "EjTBxRFTxsI0",
    "outputId": "29f72f1c-b2e3-43f4-e5b7-696c5b0334a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 29400 29401 29402]\n",
      "[    0     1     2 ... 58802 58803 58804]\n",
      "[    0     1     2 ... 88204 88205 88206]\n",
      "[     0      1      2 ... 117606 117607 117608]\n",
      "[     0      1      2 ... 147008 147009 147010]\n",
      "0        [0.021503597907142312]\n",
      "1        [0.023161837666766234]\n",
      "2        [0.018882512291082218]\n",
      "3        [0.020647733285613742]\n",
      "4        [0.021557089658616704]\n",
      "                  ...          \n",
      "29398       [0.340317312570777]\n",
      "29399     [0.34613744647139283]\n",
      "29400      [0.3459133640017863]\n",
      "29401      [0.3387979517773618]\n",
      "29402      [0.3514722195572738]\n",
      "Length: 29403, dtype: object\n",
      "0        0.021504\n",
      "1        0.023162\n",
      "2        0.018883\n",
      "3        0.020648\n",
      "4        0.021557\n",
      "           ...   \n",
      "58800    0.349949\n",
      "58801    0.339208\n",
      "58802    0.300955\n",
      "58803    0.317366\n",
      "58804    0.287858\n",
      "Length: 58805, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_x=[]\n",
    "test_x=[]\n",
    "train_y=[]\n",
    "test_y=[]\n",
    "splits=tscv.split(data_cond)\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(data_cond)):\n",
    "  print(train_index)\n",
    "  train_x.append(data_cond.iloc[train_index])\n",
    "  test_x.append(data_cond.iloc[test_index])\n",
    "  train_y.append(data_consec.iloc[train_index])\n",
    "  test_y.append(data_consec.iloc[test_index])\n",
    "\n",
    "print(train_y[0])\n",
    "for i in range(s):\n",
    "  train_y[i] = train_y[i].apply(lambda x: float(x[0]))\n",
    "  test_y[i] = test_y[i].apply(lambda x: float(x[0]))\n",
    "\n",
    "\n",
    "\n",
    "print(train_y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI7RSj_8YE-w"
   },
   "source": [
    "Ahora calculamos el score para cada uno de los conjuntos de cross validation. Como se observa a continuación los resultados obtennidos son muy similares dl orden de milésimas, pero escogeremos de todos modos el segundo que es algo mejor que el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120195,
     "status": "ok",
     "timestamp": 1679951908147,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "G7B-7ha4rqn5",
    "outputId": "fc1df6ce-0f6e-47db-f04d-091540122ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9928697588551703\n",
      "0.9932613529185266\n",
      "0.9926606943020134\n",
      "0.9923109361086543\n",
      "0.9912792530058356\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "for e in range(s):\n",
    "  regr.fit(train_x[e], train_y[e])\n",
    "\n",
    "  print(regr.score(test_x[e], test_y[e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQwkoHQguD-y"
   },
   "source": [
    "A continuación, pasamos a hacer la rejilla para calcular los hiperparámetros óptimos. Para este paso era necesario deteerminar en primer lugar cuáles sería los parámetros que optarían a ser parte de la configuración de nuestro modelo. De entre los parám"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 32173,
     "status": "error",
     "timestamp": 1679951940276,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "g13XVklZcZ9p",
    "outputId": "37ca1ac9-9171-47d0-a7d0-1e8adbbe4683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01834759704437944], [0.018240614675451242], [0.018240614675451242], [0.020540750916685547], [0.018026647669553655], [0.017491732422850875], [0.016207936057568328], [0.017919665300625456], [0.01743824067137648], [0.018722038170679623]]\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-173edb990f81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/gridSearch.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s=5\n",
    "tscv = TimeSeriesSplit(n_splits=s)\n",
    "regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = {'n_estimators': [50, 150,200,250,300],'criterion':['squared_error','friedman_mse','poisson']}\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = regr, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'neg_mean_squared_error', \n",
    "                        cv = tscv, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "\n",
    "print(y_train[20:30])\n",
    "y_train=[l[0] for l in y_train]\n",
    "model_cv.fit(x_train,y_train)\n",
    "with open('/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/gridSearch.pkl', 'wb') as file:\n",
    "        pickle.dump(model_cv, file)    \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeXK7HUz4wZ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOL0xDW-edZC"
   },
   "outputs": [],
   "source": [
    "# cv results\n",
    "with open('/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/gridSearch.pkl', 'rb') as file:\n",
    "        model_cv=pickle.load(file)     \n",
    "\n",
    "          \n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYyXxBszkn06"
   },
   "outputs": [],
   "source": [
    "sorted(cv_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuICd7BnAfs2"
   },
   "source": [
    "Ahora vamos a epresentar los resultados obtenidos para comprobar la evolución del error MAPE con los distintos parámetros. El mejor error MAPE será aquel que sea igual a 0, por tanto estamos buscando que el error sea en valor absoluto lo más cercano a 0 posible. La razón por la que tiene que ser en valor absoluto es porque el scoring que estamos usando de sklearn da los resultados del error MAPE (entre otros) en negativo.\n",
    "Vamos a representar el número de estimadores frente a la puntuación obtenida con MAPE. Es cierto que para cada punto dibujado en la gráfica si tiene en cuenta un criterio distinto, por tanto tendremos que localizar para que pareja criterio-número_de_estimadores se optiene un mejor valor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1tjIx0G_Oaa"
   },
   "outputs": [],
   "source": [
    "# plotting cv results\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.plot(cv_results[\"param_n_estimators\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_estimators\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('MAE')\n",
    "plt.title(\"Number of stimators\")\n",
    "plt.legend(['test score', 'train score'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY4TBICAmFqj"
   },
   "outputs": [],
   "source": [
    "print(cv_results[\"mean_train_score\"])\n",
    "print(cv_results[\"mean_test_score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0yNK0QTvRn_"
   },
   "source": [
    "Obtenemos el mejor valor para la siguientes configuraciones, para saber que configuración nos da los mejores resultados (aclarar que los resultados como se ven están bastante ajustados). Se propone para averiguarlo calcular la diferencia de cada par entrenamiento-test y aquella diferencia que sea la mejor de todas será la elegida como la mejor. Esto podemos permitirnoslo pues en general los resultados oscilan a el mismo valor con el orden de centésimas en la mayoría de casos.\n",
    "Se podría caer en pensar que est clase de métrica no evitaría el sobreajuste pero es cierto que si de por si para ambos conjuntos (entrenamiento y test) se consiguen valores bajos y similares querá decir que hay un buen rendimiento tanto durante el entrenamiento como durante las pruebas.\n",
    "Por ello vamos a calcular las diferencias y posteriomente selecionar a un ganador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XciLRtONAtIb"
   },
   "outputs": [],
   "source": [
    "ranking=[]\n",
    "for r in range(len(cv_results[\"mean_train_score\"])):\n",
    "  ranking.append(abs(cv_results[\"mean_train_score\"][r]-cv_results[\"mean_test_score\"][r]))\n",
    "print(ranking)\n",
    "print(ranking.index(min(ranking)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn3ivk2Mzr-b"
   },
   "source": [
    "Al usar Grid Search nos devuelve un modelo ya entrenado con la mejor configuración, por tanto, ahora probaremos con distintas configuraciones de ventanas para ver calidad de las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsytPur5A133"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/gridSearch.pkl', 'rb') as file:\n",
    "        model_cv=pickle.load(file)  \n",
    "X_test_minmax=scale(singleSerieValit)\n",
    "x,y,x_test,y_test=spliting_timeseries(X_test_minmax,5,1) #llamo al método\n",
    "cond.append(x)  #lo añado a una lista de atributos\n",
    "  # lo añado a una lista de variables predichas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_predicted=model_cv.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36GR29UxTm5P"
   },
   "source": [
    "Calculamos el MAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-6MQgomJ3JF"
   },
   "outputs": [],
   "source": [
    "#mean_squared_error(y_test, y_predicted)\n",
    "lista=list(range(0,5)[0:3])\n",
    "i=10\n",
    "f=5\n",
    "with open('/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/gridSearchmodelRandomForestw'+str(i)+'f'+str(s)+'.pkl', 'wb') as file:\n",
    "        pickle.dump(lista, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cENlfhfaTpqW"
   },
   "source": [
    "Ahora vamos a probar con distintos tamaños de ventana que error obtenemos, ara ello, una buena práctica sería ir guardando en ficheros separados todos los modelos entrenados, posteriormente calcularíamos su error, este proceso va a tardar mucho en ejecutar pues habría que entrenar un moelo por cada variación en la configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640
    },
    "executionInfo": {
     "elapsed": 22835883,
     "status": "error",
     "timestamp": 1680093460516,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "1YYSB2AWQNOv",
    "outputId": "d81c72d9-04f1-452f-eca6-ca6aabcf8ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Ahora mismo la ventana es : 10=================\n",
      "======Ahora mismo el salto es : 10===================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "p=10\n",
    "s=5\n",
    "tscv = TimeSeriesSplit(n_splits=s)\n",
    "regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = {'n_estimators': [50,150,200],'criterion':['squared_error','friedman_mse']}\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = regr, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'neg_mean_squared_error', \n",
    "                        cv = tscv, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "files=[]\n",
    "for i in range(10,288,p):\n",
    "  print(\"======Ahora mismo la ventana es : \"+str(i)+\"=================\")\n",
    "\n",
    "  print(\"======Ahora mismo el salto es : \"+str(p)+\"===================\")\n",
    "  X_train_minmax=scale(singleSerieTrT)\n",
    "  x,y,x_train,y_train=spliting_timeseries(X_train_minmax,i,1) \n",
    "\n",
    "  y_train=y_train[0:len(x_train)]\n",
    "  y_train=[l[0] for l in y_train]\n",
    "  \n",
    "\n",
    "  model_cv.fit(x_train,y_train)\n",
    "\n",
    "    \n",
    "  io.dump(model_cv, 'files/gridSearchmodelRandomForestw'+str(i)+'f'+str(s)+'.skops')\n",
    "  \n",
    "\n",
    "  if i==50:\n",
    "    p=20  \n",
    "  elif i==100:\n",
    "    p= 30\n",
    "  elif i==280:\n",
    "    p=7\n",
    "  files.append('files/gridSearchmodelRandomForest_w'+str(i)+'_f'+str(s)+'.pkl')\n",
    "  \n",
    "\n",
    "with open('fotosRandomForest/files/filesGrid.pkl', 'wb') as file:\n",
    "        pickle.dump(files, file)         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2299,
     "status": "ok",
     "timestamp": 1680187108056,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "nTYUThm-ZEca",
    "outputId": "3cd5d43a-b8fe-4681-e869-140600806d07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator GridSearchCV from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventana: 40\n",
      "Pliegues: 40\n",
      "0.06455158775640803\n",
      "Ventana: 20\n",
      "Pliegues: 20\n",
      "0.06452401165653562\n",
      "Ventana: 30\n",
      "Pliegues: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator GridSearchCV from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator GridSearchCV from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06452679745099377\n",
      "Ventana: 10\n",
      "Pliegues: 10\n",
      "0.06446690579073268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ana/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator GridSearchCV from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "directory='fotosRandomForest/files'\n",
    "files=os.listdir(directory)\n",
    "for file in files:\n",
    "    \n",
    "    io.load(directory+'/'+file, trusted=True)\n",
    "    model_cv=pickle.load(fileLoad) \n",
    "    name_split=file.split('w')\n",
    "    w=name_split[1].split('f')[0]\n",
    "    f=name_split[1].split('f')[0].split('.pkl')[0]\n",
    "\n",
    "    print(\"Ventana: \"+w)\n",
    "    print(\"Pliegues: \"+f)\n",
    "    X_test_minmax=scale(singleSerieValit)\n",
    "    x,y,x_test,y_test=spliting_timeseries(X_test_minmax,int(w),1) #llamo al método\n",
    "    y_predicted=model_cv.predict(x_test)    \n",
    "    print(mean_absolute_error(y_test, y_predicted))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPVkoZJV5gh1uHtv4u3Ihdh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
