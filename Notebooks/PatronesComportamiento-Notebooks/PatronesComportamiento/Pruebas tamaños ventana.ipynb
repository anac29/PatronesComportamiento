{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEVeaP81ywe0"
   },
   "source": [
    "# Pruebas tamaños de ventana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrLw8auxywe4"
   },
   "source": [
    "Lo que se pretende en este notebook de python en entrenar un modelo RandomForest y posteriormente uno SGDRegressor con distintos tamaños de ventana, teniendo en cuenta que cada muestra se toma cada  minutos, la ventana será inicialmete de 5 muestras (25 minutos) y se predecirá la siguiente muestra (siguientes 5 minutos), después las 10 siguientes muestras y de nuevo los siguientes 5 minutos, y así sucesivamente.\n",
    "![Vetana deslizante](https://drive.google.com/uc?export=15c-QZuq-CJLYTjf9uZvxiLF7tXWn0G2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20PNRpl4ywe5"
   },
   "source": [
    "Lo primero que tedremos que hacer será instalar todos aquellos paqutes necesarios para la ejecución del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18117,
     "status": "ok",
     "timestamp": 1681559931499,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "VlE01Q5x2PRs",
    "outputId": "212062c9-1f7a-438f-ade5-71586fbc352c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skops\n",
      "  Downloading skops-0.6.0-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.10.1\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /opt/conda/lib/python3.9/site-packages (from skops) (1.1.3)\n",
      "Collecting tabulate>=0.8.8\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.9/site-packages (from skops) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.10.1->skops) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.10.1->skops) (6.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.10.1->skops) (2022.10.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.10.1->skops) (4.4.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.10.1->skops) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=17.0->skops) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.24->skops) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.24->skops) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.24->skops) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.24->skops) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.10.1->skops) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.10.1->skops) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.10.1->skops) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.10.1->skops) (2.1.1)\n",
      "Installing collected packages: tabulate, filelock, huggingface-hub, skops\n",
      "Successfully installed filelock-3.12.0 huggingface-hub-0.14.1 skops-0.6.0 tabulate-0.9.0\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.6.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install skops\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1681561379967,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "lwh4ZnDY8fdv"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_percentage_error,mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import skops.io as io\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16770,
     "status": "ok",
     "timestamp": 1681559991701,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "IwZbogYR0hw4",
    "outputId": "6e4e0a36-0b15-4d36-acf4-71a5364adea8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmqoXToSuFKT"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2TobWEYuKAS"
   },
   "source": [
    "Estas configuraciones de ventana se harán en primer lugar para el modelo RandomForest.\n",
    "Random Forest es una de los algoritmos más populares y más comunmente usados por científicos del Dato. Random forest es un algorítmo de Machine Learning supervisado que es ampliamente utilizado y problemas de regresión y clasificación. Este crea árboles de decisión a partir de diferentes muestras y toma su voto mayoritario para clasificación y su aproximación para regresión.\n",
    "\n",
    "Una de las características más importantes del algoritmo Random Forest es que puede soportar data sets con variables contínuas, como es el caso de la regresión, y variables categóricas como es el caso de la clasificación.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9JuxR5X2u6c"
   },
   "source": [
    "El sobreajuste suele ser un problema crítico que puede empeorar los resultafos obtenidos pero para el algortimo random forest, si hay suficientes árboles en el bosque el casificador con reajustará el modelo. Otro punto a favor de este algoritmo es que puede soportar valores vacíos, no obstante para nuestro ejemplo no trataremos con ellos pues como se muestra en el jupyter [Transformación de Datos](https://colab.research.google.com/drive/13Bu4aIENRpZTCj9K0ozJAoSONLPTK5jI?usp=sharing), se han reconstruido todos aquellos tramos horarios en los que había cortes de luz y se perdían muetras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcsNcwKh4Aiu"
   },
   "source": [
    "# Cómo funciona este algoritmo\n",
    "Para explicarlo nos basaremos en la siguiente ilustación. Como se observa en la imagen inferior observamos como a partir de los dígitos iniciales que vienen subrayados y en distintos colores el árbol va tomando decisiones y categorizándolo. Tenemos el siguiente número 1 1 0 0 0 0 0, que presenta distintos colores y algunos de ellos están subrayados y otros no. Queremos categorizar dicho número en: rojo, azul y subrayado. El funcionamiento de un árbol de decisión viene definido por un esquema, la primera pregunta clasificará a aquellos números que sean de color rojo por una lado y los de otro color por otro, para la siguiente clasificación solo se tiene en cuenta el conjunto de la izquierda pues el de la derecha ya ha llegado a una categorá final: azul. Siguiendo con el conjunto de la izquierda podemos categorizar en subrayado y no subrayado, por tanto tendremos a la izquierda los unos subrayados y a la derecha el cero sin subrayar. Para nuestro problema es algo más complejo pero entendiendo las bases se puede estrapolar perfectamente a casos de mayor embergadura.\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1hoYiU6y5bpVt6yS51cdKbHUfxiDzMBP4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2frIZPyZgDa8"
   },
   "source": [
    "Pero en este caso no estaríamos hablando de un único árbol de decisión si no de muchos árboles que conformarán 'El bosque', de ahí el nombre del algoritmo. Estos arbolitos trabajan como un conjunto y este comportamiento es la clave de su  baja correlación y en consecuencia su mejor rendimiento frente a algorítmos con una alta correlación. Cada árbol da una predicción de forma que el impacto de la predicción se divide entre el número de árboles totales, de forma que la predicción final está protegida de los pequeños errores individuales. Uso por tanto la sabiduría de masas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BEkgxbihiWN"
   },
   "source": [
    "# Transformar serie temporal en ventanas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIdKpevWR1pu"
   },
   "source": [
    "Iremos modificando los tamaños de ventana y comprobando que resultados se van obteniendo tras entrenar el modelo con la mejor configuración de validación cruzada. Primero tomaremos con pickle la serie que contiene todas las series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ruta del directorio de trabajo actual es: /home/user/work/PatronesComportamiento/PatronesComportamiento\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ruta_actual = os.getcwd()\n",
    "print(\"La ruta del directorio de trabajo actual es:\", ruta_actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1796,
     "status": "ok",
     "timestamp": 1681560014300,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "U4E5JuAHP8_4",
    "outputId": "b84a4eb9-c138-4f00-d4ec-1d319287b021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.288288, 1.207547, 1.316038, 1.264151, 1.132075, 0.8962264, 1.042453, 0.6650943, 0.8207547, 0.9009434]\n",
      "287\n"
     ]
    }
   ],
   "source": [
    "with open('/home/user/work/PatronesComportamiento/PatronesComportamiento/mySeriesInten.pkl', 'rb') as file:\n",
    "    \n",
    "    timeSeries = pickle.load(file)\n",
    "\n",
    "with open('/home/user/work/PatronesComportamiento/PatronesComportamiento/singleSerieIntenTrT.pkl', 'rb') as file:\n",
    "    singleSerieTrT = pickle.load(file)\n",
    "\n",
    "\n",
    "with open('/home/user/work/PatronesComportamiento/PatronesComportamiento/singleSerieIntenValit.pkl', 'rb') as file:\n",
    "    singleSerieValit= pickle.load(file)\n",
    "\n",
    "print((singleSerieTrT[:10]))\n",
    "print(len(timeSeries[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kO5ShdQgsE7"
   },
   "source": [
    "Lo primero que vamos a hacer es escalar los datos con sk learn en su paquete [Preprocesing](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) tiene alguns métodos que permiten escalar los datos de entrenamiento según distintos criterios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1681560015460,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "pJZH_UnpgrUl"
   },
   "outputs": [],
   "source": [
    "def scale(data):  \n",
    "  min_max_scaler = preprocessing.MinMaxScaler()\n",
    "  X_train=np.array(data)\n",
    "  X_train = X_train.reshape((len(data), 1))\n",
    "  X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "  X_train_minmax=[ele[0] for ele in X_train_minmax]\n",
    "  return X_train_minmax,min_max_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp0WE7Tch6b4"
   },
   "source": [
    "En primer lugar aplicaremos una ventana a los datos, en este caso la ventana será de 5 y la h de 1. La forma que hemos elegido para aplicar la ventana deslizante es a través de sliding vectorial, posteriomente se añade a un dataframe de pandas. A continuación en la celda contigua podemos ver la definición del método que aplicará la ventana deslizante. Para entender el por qué de utilizar esta técnica ir al jupyter [Transformar datos para prediccion](https://colab.research.google.com/drive/1HuCj1Cetas3ffaXV8q83ynGQaV_njtI6?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681560016815,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "G7TLr3aiTb1T"
   },
   "outputs": [],
   "source": [
    "def spliting_timeseries(timeSerie,w,h):  #w amplitud de la ventana y h >1 si es multiobjetivo\n",
    "  X, y = [], []  #inicializamos los arrays que guardarán atributos y variable predicha\n",
    "  for i in range(len(timeSerie) - w): #bucle para iterar sobre todas las muestras de las series \n",
    "    X.append(timeSerie[i : i + w])  #cogemos desde el elemento en el que estamos hasta los w siguientes\n",
    "    y.append(timeSerie[i + w:i+w+h])  #aquí cogemos desde el w+1 hasta los h siguientes\n",
    "  return pd.DataFrame(X), pd.Series(y),X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt_JVpjLif_J"
   },
   "source": [
    "Lo siguiente será llamar al método con aquellos parámetros con los que queramos configurar la ventana deslizante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1681560878880,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "W8jxq-5BTgq7"
   },
   "outputs": [],
   "source": [
    "def prepare_data(singleSerieTrT,w,h):\n",
    "  X_train_minmax=scale(singleSerieTrT)\n",
    "  cond,consec=[],[]\n",
    "  x,y,x_train,y_train=spliting_timeseries(X_train_minmax,5,1) #llamo al método\n",
    "  cond.append(x)  #lo añado a una lista de atributos\n",
    "    # lo añado a una lista de variables predichas\n",
    "\n",
    "\n",
    "\n",
    "  data_cond = pd.concat(cond, axis=0) # combino todos los DataFrames en uno solo\n",
    "  data_consec = pd.concat([y], axis=0) # combino todos los DataFrames en uno solo\n",
    "  return x,y,x_train,y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjGiwxXfjEOQ"
   },
   "source": [
    "El siguiente paso a abordar será dividir los datos en entrenamiento y test. Algo que debemos tener en cuenta a la hora de hacer esta división es que nuestro objeto de estudio son series temporales, por tanto debemos plantear una for de dividir los datos sin que afecte a la línea temporal de los mismos. (Explicar con detalle cuando lea información al respecto).\n",
    "Para que la división utilizada respete la cronología se utilizará el método: [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit) de la librería SK Learn. Este método permite aplicar validación cruzada a series temporales respectando la cronología, Para este método si k es el número de pligues devolverá el pliegue k como el último conjunto que formaŕá parte del conjunto de entrenamiento y el k+1 como el primero del conjunto de test. A diferencia de los métodos cotidianos de validación cruzada los conjuntos sucesivos de entrenamiento son superconjuntos de los conjuntos de entrenamiento previos. Através de este método obtendrems los índices de los conjuntos que formarán parte del entrenamiento y de los que formarán parte del test.\n",
    "De entre los dintintos parámtros configurables en le método tan sólo configuraremos n_splits a 5, es meramente informativo pues es el valor por defecto del parámetro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1681552396385,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "sZSO0FGXTjZ2",
    "outputId": "bbb41f37-ffe2-4b53-aad4-c459d4dd6124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_cond' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tscv)\n\u001b[1;32m      5\u001b[0m fols_dict\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (train_index, test_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tscv\u001b[38;5;241m.\u001b[39msplit(\u001b[43mdata_cond\u001b[49m)):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     fols_dict[i]\u001b[38;5;241m=\u001b[39m[train_index,test_index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_cond' is not defined"
     ]
    }
   ],
   "source": [
    "s=5\n",
    "tscv = TimeSeriesSplit(n_splits=s)\n",
    "print(tscv)\n",
    "\n",
    "fols_dict={}\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(data_cond)):\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    fols_dict[i]=[train_index,test_index]\n",
    "    \n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "print(data_cond.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJJHbUilsNTZ"
   },
   "source": [
    "Ahora tenemos distintos conjuntos de entrenamiento y test, de cara al siguiente paso tendremos que tener elegida una configuaración, para ello calcularemos el score que se obtiene para cada una de ellas y aquel que consiga un mayor valor será selecionado. No obstante, aún quedan configuraciones que hacer antes de conseguir el conjunto de entrenamiento y test así como el modelo final.\n",
    "Tenemos que configurar los distintos conjuntos de entrenamiento y test y probar su rendimiento con la fución score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1681552398349,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "EjTBxRFTxsI0",
    "outputId": "022d17c5-e88b-4a4a-b70e-95a7f3d407e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 29400 29401 29402]\n",
      "[    0     1     2 ... 58802 58803 58804]\n",
      "[    0     1     2 ... 88204 88205 88206]\n",
      "[     0      1      2 ... 117606 117607 117608]\n",
      "[     0      1      2 ... 147008 147009 147010]\n",
      "0        [0.021503597907142312]\n",
      "1        [0.023161837666766234]\n",
      "2        [0.018882512291082218]\n",
      "3        [0.020647733285613742]\n",
      "4        [0.021557089658616704]\n",
      "                  ...          \n",
      "29398       [0.340317312570777]\n",
      "29399     [0.34613744647139283]\n",
      "29400      [0.3459133640017863]\n",
      "29401      [0.3387979517773618]\n",
      "29402      [0.3514722195572738]\n",
      "Length: 29403, dtype: object\n",
      "0        0.021504\n",
      "1        0.023162\n",
      "2        0.018883\n",
      "3        0.020648\n",
      "4        0.021557\n",
      "           ...   \n",
      "58800    0.349949\n",
      "58801    0.339208\n",
      "58802    0.300955\n",
      "58803    0.317366\n",
      "58804    0.287858\n",
      "Length: 58805, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_x=[]\n",
    "test_x=[]\n",
    "train_y=[]\n",
    "test_y=[]\n",
    "splits=tscv.split(data_cond)\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(data_cond)):\n",
    "  print(train_index)\n",
    "  train_x.append(data_cond.iloc[train_index])\n",
    "  test_x.append(data_cond.iloc[test_index])\n",
    "  train_y.append(data_consec.iloc[train_index])\n",
    "  test_y.append(data_consec.iloc[test_index])\n",
    "\n",
    "print(train_y[0])\n",
    "for i in range(s):\n",
    "  train_y[i] = train_y[i].apply(lambda x: float(x[0]))\n",
    "  test_y[i] = test_y[i].apply(lambda x: float(x[0]))\n",
    "\n",
    "\n",
    "\n",
    "print(train_y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI7RSj_8YE-w"
   },
   "source": [
    "Ahora calculamos el score para cada uno de los conjuntos de cross validation. Como se observa a continuación los resultados obtennidos son muy similares dl orden de milésimas, pero escogeremos de todos modos el segundo que es algo mejor que el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120195,
     "status": "ok",
     "timestamp": 1679951908147,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "G7B-7ha4rqn5",
    "outputId": "fc1df6ce-0f6e-47db-f04d-091540122ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9928697588551703\n",
      "0.9932613529185266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m regr \u001b[38;5;241m=\u001b[39m RandomForestRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(s):\n\u001b[1;32m----> 3\u001b[0m   \u001b[43mregr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(regr\u001b[38;5;241m.\u001b[39mscore(test_x[e], test_y[e]))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "for e in range(s):\n",
    "  regr.fit(train_x[e], train_y[e])\n",
    "\n",
    "  print(regr.score(test_x[e], test_y[e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQwkoHQguD-y"
   },
   "source": [
    "A continuación, pasamos a hacer la rejilla para calcular los hiperparámetros óptimos. Para este paso era necesario deteerminar en primer lugar cuáles sería los parámetros que optarían a ser parte de la configuración de nuestro modelo. De entre los parám"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuICd7BnAfs2"
   },
   "source": [
    "Ahora vamos a representar los resultados obtenidos para comprobar la evolución del error MAPE con los distintos parámetros. El mejor error MAPE será aquel que sea igual a 0, por tanto estamos buscando que el error sea en valor absoluto lo más cercano a 0 posible. La razón por la que tiene que ser en valor absoluto es porque el scoring que estamos usando de sklearn da los resultados del error MAPE (entre otros) en negativo.\n",
    "Vamos a representar el número de estimadores frente a la puntuación obtenida con MAPE. Es cierto que para cada punto dibujado en la gráfica si tiene en cuenta un criterio distinto, por tanto tendremos que localizar para que pareja criterio-número_de_estimadores se optiene un mejor valor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0yNK0QTvRn_"
   },
   "source": [
    "Obtenemos el mejor valor para la siguientes configuraciones, para saber que configuración nos da los mejores resultados (aclarar que los resultados como se ven están bastante ajustados). Se propone para averiguarlo calcular la diferencia de cada par entrenamiento-test y aquella diferencia que sea la mejor de todas será la elegida como la mejor. Esto podemos permitirnoslo pues en general los resultados oscilan a el mismo valor con el orden de centésimas en la mayoría de casos.\n",
    "Se podría caer en pensar que est clase de métrica no evitaría el sobreajuste pero es cierto que si de por si para ambos conjuntos (entrenamiento y test) se consiguen valores bajos y similares querá decir que hay un buen rendimiento tanto durante el entrenamiento como durante las pruebas.\n",
    "Por ello vamos a calcular las diferencias y posteriomente selecionar a un ganador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XciLRtONAtIb"
   },
   "outputs": [],
   "source": [
    "ranking=[]\n",
    "for r in range(len(cv_results[\"mean_train_score\"])):\n",
    "  ranking.append(abs(cv_results[\"mean_train_score\"][r]-cv_results[\"mean_test_score\"][r]))\n",
    "print(ranking)\n",
    "print(ranking.index(min(ranking)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn3ivk2Mzr-b"
   },
   "source": [
    "Al usar Grid Search nos devuelve un modelo ya entrenado con la mejor configuración, por tanto, ahora probaremos con distintas configuraciones de ventanas para ver calidad de las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsytPur5A133"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/gridSearch.pkl', 'rb') as file:\n",
    "        model_cv=pickle.load(file)  \n",
    "X_test_minmax=scale(singleSerieValit)\n",
    "x,y,x_test,y_test=spliting_timeseries(X_test_minmax,5,1) #llamo al método\n",
    "cond.append(x)  #lo añado a una lista de atributos\n",
    "  # lo añado a una lista de variables predichas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_predicted=model_cv.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36GR29UxTm5P"
   },
   "source": [
    "Calculamos el MAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-6MQgomJ3JF"
   },
   "outputs": [],
   "source": [
    "#mean_squared_error(y_test, y_predicted)\n",
    "lista=list(range(0,5)[0:3])\n",
    "i=10\n",
    "f=5\n",
    "with open('/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/gridSearchmodelRandomForestw'+str(i)+'f'+str(s)+'.pkl', 'wb') as file:\n",
    "        pickle.dump(lista, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cENlfhfaTpqW"
   },
   "source": [
    "Ahora vamos a probar con distintos tamaños de ventana que error obtenemos, ara ello, una buena práctica sería ir guardando en ficheros separados todos los modelos entrenados, posteriormente calcularíamos su error, este proceso va a tardar mucho en ejecutar pues habría que entrenar un moelo por cada variación en la configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640
    },
    "executionInfo": {
     "elapsed": 22835883,
     "status": "error",
     "timestamp": 1680093460516,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "1YYSB2AWQNOv",
    "outputId": "d81c72d9-04f1-452f-eca6-ca6aabcf8ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Ahora mismo la ventana es : 60=================\n",
      "======Ahora mismo el salto es : 10===================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "p=10\n",
    "s=5\n",
    "tscv = TimeSeriesSplit(n_splits=s)\n",
    "regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = {'n_estimators': [50,150,200],'criterion':['squared_error','friedman_mse']}\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = regr, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'neg_mean_squared_error', \n",
    "                        cv = tscv, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "files=[]\n",
    "for i in range(60,288,p):\n",
    "  print(\"======Ahora mismo la ventana es : \"+str(i)+\"=================\")\n",
    "\n",
    "  print(\"======Ahora mismo el salto es : \"+str(p)+\"===================\")\n",
    "  X_train_minmax=scale(singleSerieTrT)\n",
    "  x,y,x_train,y_train=spliting_timeseries(X_train_minmax,i,1) \n",
    "\n",
    "  y_train=y_train[0:len(x_train)]\n",
    "  y_train=[l[0] for l in y_train]\n",
    "  \n",
    "\n",
    "  model_cv.fit(x_train,y_train)\n",
    "\n",
    "    \n",
    "  io.dump(model_cv, 'fotosRandomForest/files/VentanagridSearchmodelRandomForestw'+str(i)+'f'+str(s)+'.skops')\n",
    "  \n",
    "\n",
    "  if i==50:\n",
    "    p=20  \n",
    "  elif i==100:\n",
    "    p= 30\n",
    "  elif i==280:\n",
    "    p=7\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73313,
     "status": "ok",
     "timestamp": 1681292435711,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "nTYUThm-ZEca",
    "outputId": "c86d4cb0-bdbe-40c7-e98f-5ae9d9be67d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventana: 240\n",
      "0.06514649583432994\n",
      "Ventana: 10\n",
      "0.06446690579073268\n",
      "Ventana: 270\n",
      "0.06522446610833704\n",
      "Ventana: 50\n",
      "0.0645312776663096\n",
      "Ventana: 90\n",
      "0.06465631287320467\n",
      "Ventana: 40\n",
      "0.06455158775640803\n",
      "Ventana: 288\n",
      "0.06531306357822554\n",
      "Ventana: 100\n",
      "0.06475366269641983\n",
      "Ventana: 120\n",
      "0.06477284412094883\n",
      "Ventana: 70\n",
      "0.06454804145508791\n",
      "Ventana: 140\n",
      "0.06486281464337555\n",
      "Ventana: 180\n",
      "0.06492949053019263\n",
      "Ventana: 130\n",
      "0.06472420528595953\n",
      "Ventana: 20\n",
      "0.06452401165653562\n",
      "Ventana: 150\n",
      "0.06467707689119896\n",
      "Ventana: 60\n",
      "0.06462280273435314\n",
      "Ventana: 110\n",
      "0.06470686043996093\n",
      "Ventana: 80\n",
      "0.06465513821663608\n",
      "Ventana: 210\n",
      "0.0650434565900811\n",
      "Ventana: 30\n",
      "0.06452679745099377\n"
     ]
    }
   ],
   "source": [
    "directory='/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/files'\n",
    "files=os.listdir(directory)\n",
    "for file in files:\n",
    "    \n",
    "    model_cv=io.load(directory+'/'+file, trusted=True)\n",
    "    name_split=file.split('w')\n",
    "    w=name_split[1].split('f')[0]\n",
    "    f=name_split[1].split('f')[0].split('.skops')[0]\n",
    "\n",
    "    print(\"Ventana: \"+w)\n",
    "    X_test_minmax=scale(singleSerieValit)\n",
    "    x,y,x_test,y_test=spliting_timeseries(X_test_minmax,int(w),1) #llamo al método\n",
    "    y_predicted=model_cv.predict(x_test)    \n",
    "    print(mean_absolute_error(y_test, y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AHZGhvaz_j_"
   },
   "source": [
    "Ahora las predicciones las haremos para un mayor valor de h se forma que vamos a predecir más muestras. Usaremos a librería MultiOutputRegressor de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 24935,
     "status": "error",
     "timestamp": 1681561654189,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "hyKS_lhz01us",
    "outputId": "69f4fcbf-7d3a-4b41-ed24-4b16c681bcb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Ahora mismo la ventana es : 288=================\n",
      "======Ahora mismo el salto es : 72===================\n",
      "288\n",
      "72\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p=10\n",
    "s=5\n",
    "tscv = TimeSeriesSplit(n_splits=s)\n",
    "regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = {'estimator__n_estimators': [200],'estimator__criterion':['squared_error']}\n",
    "\n",
    "# definir el metaestimador de múltiples objetivos\n",
    "clf = MultiOutputRegressor(regr)\n",
    "\n",
    "# definir el modelo con búsqueda de cuadrícula\n",
    "model_cv = GridSearchCV(estimator=clf,\n",
    "                        param_grid=hyper_params,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=tscv,\n",
    "                        verbose=1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# fit the model\n",
    "files=[]\n",
    "wh=[[288,72]]\n",
    "for pack in wh:\n",
    "  print(\"======Ahora mismo la ventana es : \"+str(pack[0])+\"=================\")\n",
    "\n",
    "  print(\"======Ahora mismo el salto es : \"+str(pack[1])+\"===================\")\n",
    "  X_train_minmax,min_max_scaler=scale(singleSerieTrT)\n",
    "  print(pack[0])\n",
    "  print(pack[1])\n",
    "  x,y,x_train,y_train=spliting_timeseries(X_train_minmax,pack[0],pack[1]) \n",
    "  y_train=y_train[0:len(x_train)]\n",
    "  y_train_last=[]\n",
    "  for conj in y_train:\n",
    "    if len(conj)==pack[1]:\n",
    "      y_train_last.append(conj)\n",
    "  max=len(y_train_last)\n",
    "\n",
    "\n",
    "  model_cv.fit(x_train[:max],y_train_last)\n",
    "\n",
    "    \n",
    "  io.dump(model_cv, '/home/user/work/PatronesComportamiento/PatronesComportamiento/fotosRandomForest/filesHwide/VentanagridSearchmodelSGDRegressorw'+str(pack[0])+'f'+str(pack[1])+'.skops')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvfW1sFIywfE"
   },
   "source": [
    "# SGD Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfeoHEs7ywfF"
   },
   "source": [
    "Ahora tendremos que hacer lo mismo para este otro modelo, en pruebas anteriores que se pueden encontrar en el notebook [SGDREgressor](https://colab.research.google.com/drive/1RTkMuPEW5UMJ0gCuaGMnELhT0u1TYRUq?usp=sharing). Lo métodos y están definidos, ahora sólo hay que entrenar los modelos para este algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 608243,
     "status": "ok",
     "timestamp": 1681553659102,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "EU_84x2PywfF",
    "outputId": "b2d9b66d-b5bc-4156-b917-fe594e9be8f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Ahora mismo la ventana es : 10=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 20=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 30=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 40=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 50=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 70=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 90=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 110=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 140=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 170=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 200=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 230=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 260=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "======Ahora mismo la ventana es : 288=================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "p=10\n",
    "\n",
    "s=5\n",
    "tscv = TimeSeriesSplit(n_splits=s)\n",
    "regr =  SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = {'loss': ['squared_error','huber','epsilon_insensitive'],'penalty':['elasticnet','l1']}\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = regr, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'neg_mean_squared_error', \n",
    "                        cv = tscv, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "files=[]\n",
    "sizes=[10,20,30,40,50,70,90,110,140,170,200,230,260,288]\n",
    "for i in sizes:\n",
    "  print(\"======Ahora mismo la ventana es : \"+str(i)+\"=================\")\n",
    "\n",
    "  X_train_minmax=scale(singleSerieTrT)\n",
    "  x,y,x_train,y_train=spliting_timeseries(X_train_minmax,i,1) \n",
    "\n",
    "  y_train=y_train[0:len(x_train)]\n",
    "  y_train=[l[0] for l in y_train]\n",
    "  \n",
    "\n",
    "  model_cv.fit(x_train,y_train)\n",
    "\n",
    "    \n",
    "  io.dump(model_cv, '/content/gdrive/MyDrive/PatronesComportamiento/filesSGD/VentanagridSearchmodelSGDRegressorw'+str(i)+'f'+str(s)+'.skops')\n",
    "  \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28653,
     "status": "ok",
     "timestamp": 1681292591196,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "Ufm3Vukd2CIB",
    "outputId": "87d1d22c-6b9b-455a-9f05-de601b32d615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventana: 200\n",
      "0.020500522302234946\n",
      "Ventana: 140\n",
      "0.021079111825852384\n",
      "Ventana: 288\n",
      "0.01875494118552419\n",
      "Ventana: 230\n",
      "0.019722425513049046\n",
      "Ventana: 10\n",
      "0.01968515876198442\n",
      "Ventana: 70\n",
      "0.019784449713479146\n",
      "Ventana: 90\n",
      "0.021087580596705065\n",
      "Ventana: 20\n",
      "0.020565666053166107\n",
      "Ventana: 40\n",
      "0.01973199213741264\n",
      "Ventana: 110\n",
      "0.019949519343395637\n",
      "Ventana: 260\n",
      "0.019474092797366838\n",
      "Ventana: 170\n",
      "0.021531222576135135\n",
      "Ventana: 30\n",
      "0.01988129320528043\n",
      "Ventana: 50\n",
      "0.020017535474110684\n"
     ]
    }
   ],
   "source": [
    "directory='/content/gdrive/MyDrive/PatronesComportamiento/filesSGD'\n",
    "files=os.listdir(directory)\n",
    "for file in files:\n",
    "    \n",
    "    model_cv=io.load(directory+'/'+file, trusted=True)\n",
    "    name_split=file.split('w')\n",
    "    w=name_split[1].split('f')[0]\n",
    "    f=name_split[1].split('f')[0].split('.skops')[0]\n",
    "\n",
    "    print(\"Ventana: \"+w)\n",
    "    X_test_minmax=scale(singleSerieValit)\n",
    "    x,y,x_test,y_test=spliting_timeseries(X_test_minmax,int(w),1) #llamo al método\n",
    "    y_predicted=model_cv.predict(x_test)    \n",
    "    print(mean_absolute_error(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNeBLeRG5jch"
   },
   "source": [
    "\n",
    "De nuevo tenemos que entrenar los modelos variando el tamaño de la h, pero esta vez con SGDREgressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 21339,
     "status": "error",
     "timestamp": 1681562215242,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "DLRBv4tm6L1v",
    "outputId": "d423251b-a46c-47e6-be19-dfcb386fb9df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Ahora mismo la ventana es : 288=================\n",
      "======Ahora mismo el salto es : 72===================\n",
      "288\n",
      "72\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "p=10\n",
    "s=5\n",
    "tscv = TimeSeriesSplit(n_splits=s)\n",
    "regr =  SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# step-1: create a cross-validation scheme\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = {'estimator__loss': ['squared_error', 'huber', 'epsilon_insensitive'], 'estimator__penalty': ['elasticnet', 'l1']}\n",
    "\n",
    "# definir el metaestimador de múltiples objetivos\n",
    "clf = MultiOutputRegressor(regr)\n",
    "\n",
    "# definir el modelo con búsqueda de cuadrícula\n",
    "model_cv = GridSearchCV(estimator=clf,\n",
    "                        param_grid=hyper_params,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=tscv,\n",
    "                        verbose=1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# fit the model\n",
    "files=[]\n",
    "wh=[[288,72],[288,36]]\n",
    "for pack in wh:\n",
    "  print(\"======Ahora mismo la ventana es : \"+str(pack[0])+\"=================\")\n",
    "\n",
    "  print(\"======Ahora mismo el salto es : \"+str(pack[1])+\"===================\")\n",
    "  X_train_minmax,min_max_scaler=scale(singleSerieTrT)\n",
    "  print(pack[0])\n",
    "  print(pack[1])\n",
    "  x,y,x_train,y_train=spliting_timeseries(X_train_minmax,pack[0],pack[1]) \n",
    "  y_train=y_train[0:len(x_train)]\n",
    "  y_train_last=[]\n",
    "  for conj in y_train:\n",
    "    if len(conj)==pack[1]:\n",
    "      y_train_last.append(conj)\n",
    "  max=len(y_train_last)\n",
    "\n",
    " \n",
    "\n",
    "  model_cv.fit(x_train[:max],y_train_last)\n",
    "\n",
    "    \n",
    "  io.dump(model_cv,'/home/user/work/PatronesComportamiento/PatronesComportamiento/filesSGD/VentanagridSearchmodelSGDRegressorw'+str(pack[0])+'f'+str(pack[1])+'.skops')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "  io.dump(model_cv,'/home/user/work/PatronesComportamiento/PatronesComportamiento/filesSGD/VentanagridSearchmodelSGDRegressorw'+str(pack[0])+'f'+str(pack[1])+'.skops')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nVkz2532I47"
   },
   "outputs": [],
   "source": [
    "randomForest=[]\n",
    "\n",
    "\n",
    "directory='/content/gdrive/MyDrive/PatronesComportamiento/fotosRandomForest/files'\n",
    "files=os.listdir(directory)\n",
    "for file in files:\n",
    "    model_cv=io.load(directory+'/'+file, trusted=True)\n",
    "    w=file.split('w')[1].split('f')[0]\n",
    "    f=file.split('f')[1].split('.skops')[0]\n",
    "    criterion=model_cv.best_params_['criterion']\n",
    "    n_estimators=model_cv.best_params_['n_estimators']\n",
    "    X_test_minmax=scale(singleSerieValit)\n",
    "    x,y,x_test,y_test=spliting_timeseries(X_test_minmax,int(w),1) #llamo al método\n",
    "    y_predicted=model_cv.predict(x_test)    \n",
    "    MAE=mean_absolute_error(y_test, y_predicted)\n",
    "    randomForest.append(['randomForest',w,n_estimators,criterion,MAE])\n",
    "  \n",
    "randomForest=sorted(randomForest, key=lambda e: e[4])\n",
    "dfRandom = pd.DataFrame(np.array(randomForest),\n",
    "                   columns=['algoritmo','ventana','n_estimators','criterion','MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1681323175444,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "7xPUc2lhxC1i",
    "outputId": "96710089-bc20-4456-e306-e565f779a77a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f9fa27bc-7fbb-4052-a42d-01ed5184bd93\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algoritmo</th>\n",
       "      <th>ventana</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>criterion</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.06446690579073268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>20</td>\n",
       "      <td>150</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06452401165653562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06452679745099377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>50</td>\n",
       "      <td>150</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.0645312776663096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>70</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06454804145508791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>40</td>\n",
       "      <td>150</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06455158775640803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.06462280273435314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>80</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06465513821663608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.06465631287320467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06467707689119896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>110</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06470686043996093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>130</td>\n",
       "      <td>200</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.06472420528595953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06475366269641983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.06477284412094883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>140</td>\n",
       "      <td>200</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.06486281464337555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>180</td>\n",
       "      <td>150</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.06492949053019263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>210</td>\n",
       "      <td>200</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.0650434565900811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>240</td>\n",
       "      <td>200</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>0.06514649583432994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>270</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06522446610833704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>randomForest</td>\n",
       "      <td>288</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>0.06531306357822554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9fa27bc-7fbb-4052-a42d-01ed5184bd93')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f9fa27bc-7fbb-4052-a42d-01ed5184bd93 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f9fa27bc-7fbb-4052-a42d-01ed5184bd93');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       algoritmo ventana n_estimators      criterion                  MAE\n",
       "0   randomForest      10          150   friedman_mse  0.06446690579073268\n",
       "1   randomForest      20          150  squared_error  0.06452401165653562\n",
       "2   randomForest      30          200  squared_error  0.06452679745099377\n",
       "3   randomForest      50          150   friedman_mse   0.0645312776663096\n",
       "4   randomForest      70          200  squared_error  0.06454804145508791\n",
       "5   randomForest      40          150  squared_error  0.06455158775640803\n",
       "6   randomForest      60          150   friedman_mse  0.06462280273435314\n",
       "7   randomForest      80          200  squared_error  0.06465513821663608\n",
       "8   randomForest      90          200   friedman_mse  0.06465631287320467\n",
       "9   randomForest     150          200  squared_error  0.06467707689119896\n",
       "10  randomForest     110          200  squared_error  0.06470686043996093\n",
       "11  randomForest     130          200   friedman_mse  0.06472420528595953\n",
       "12  randomForest     100          200  squared_error  0.06475366269641983\n",
       "13  randomForest     120          200   friedman_mse  0.06477284412094883\n",
       "14  randomForest     140          200   friedman_mse  0.06486281464337555\n",
       "15  randomForest     180          150   friedman_mse  0.06492949053019263\n",
       "16  randomForest     210          200   friedman_mse   0.0650434565900811\n",
       "17  randomForest     240          200   friedman_mse  0.06514649583432994\n",
       "18  randomForest     270          200  squared_error  0.06522446610833704\n",
       "19  randomForest     288          200  squared_error  0.06531306357822554"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 18164,
     "status": "ok",
     "timestamp": 1681560065801,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "U30I3kHEyNdz"
   },
   "outputs": [],
   "source": [
    "SGDregres=[]\n",
    "\n",
    "\n",
    "directory='/content/gdrive/MyDrive/PatronesComportamiento/filesSGD'\n",
    "files=os.listdir(directory)\n",
    "for file in files:\n",
    "    w=file.split('w')[1].split('f')[0]\n",
    "    f=file.split('f')[1].split('.skops')[0]\n",
    "    model_cv=io.load(directory+'/'+file, trusted=True)\n",
    "    loss=model_cv.best_params_['loss']\n",
    "    penalty=model_cv.best_params_['penalty']\n",
    "    X_test_minmax=scale(singleSerieValit)\n",
    "    x,y,x_test,y_test=spliting_timeseries(X_test_minmax,int(w),1) #llamo al método\n",
    "    y_predicted=model_cv.predict(x_test)    \n",
    "    MAE=mean_absolute_error(y_test, y_predicted)\n",
    "    SGDregres.append(['SGDRegressor',w,loss,penalty,MAE])\n",
    "    #model_cv=io.load(directory+'/'+file, trusted=True)\n",
    "    \n",
    "SGDregres=sorted(SGDregres, key=lambda e: e[4])\n",
    "df2SGD = pd.DataFrame(np.array(SGDregres),\n",
    "                   columns=['algoritmo','ventana','loss','penalty','MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1681560069964,
     "user": {
      "displayName": "Ana Conde",
      "userId": "00740142815161481389"
     },
     "user_tz": -120
    },
    "id": "1dimmY51kCVo",
    "outputId": "695cc98c-e5dc-46aa-c452-428c71963d54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-20b4ab79-390a-4620-9418-228be69f09af\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algoritmo</th>\n",
       "      <th>ventana</th>\n",
       "      <th>loss</th>\n",
       "      <th>penalty</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>288</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.018724408794318602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>260</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.019342111233262467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>70</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.019730354040509684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>30</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.019820571426825376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>230</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.01982437479666238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>10</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.019888525845409764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>40</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.019895035224806526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>50</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.019975728115561503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>110</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.02011388925509061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>200</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.020193674485808024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>20</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.020201902979813915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>90</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.020488561618049527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>140</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.0206419838742687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>170</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.02185938171325985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20b4ab79-390a-4620-9418-228be69f09af')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-20b4ab79-390a-4620-9418-228be69f09af button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-20b4ab79-390a-4620-9418-228be69f09af');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       algoritmo ventana           loss     penalty                   MAE\n",
       "0   SGDRegressor     288  squared_error  elasticnet  0.018724408794318602\n",
       "1   SGDRegressor     260  squared_error  elasticnet  0.019342111233262467\n",
       "2   SGDRegressor      70  squared_error  elasticnet  0.019730354040509684\n",
       "3   SGDRegressor      30  squared_error  elasticnet  0.019820571426825376\n",
       "4   SGDRegressor     230  squared_error  elasticnet   0.01982437479666238\n",
       "5   SGDRegressor      10  squared_error  elasticnet  0.019888525845409764\n",
       "6   SGDRegressor      40  squared_error  elasticnet  0.019895035224806526\n",
       "7   SGDRegressor      50  squared_error  elasticnet  0.019975728115561503\n",
       "8   SGDRegressor     110  squared_error  elasticnet   0.02011388925509061\n",
       "9   SGDRegressor     200  squared_error  elasticnet  0.020193674485808024\n",
       "10  SGDRegressor      20  squared_error          l1  0.020201902979813915\n",
       "11  SGDRegressor      90  squared_error  elasticnet  0.020488561618049527\n",
       "12  SGDRegressor     140  squared_error  elasticnet    0.0206419838742687\n",
       "13  SGDRegressor     170  squared_error  elasticnet   0.02185938171325985"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qt2kWtkRmu43"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
